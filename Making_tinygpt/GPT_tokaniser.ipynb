{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73acacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3063dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokanisation is nothing but making it encripted and in this we will learn byte pair encoding insted of \n",
    "#conventional way of just giving out character encoding method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e05c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115, 111, 109, 101, 116, 104, 105, 110, 103]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so how do the computer understand a character the answer is unicode \n",
    "#which has around 160000 character in itself including all the emojies and other languages \n",
    "#here is a representation\n",
    "ord(\"4\")# it only takes in one character insted of a whole string so when ur trying to use string use a for loop like this \n",
    "[ord(x) for x in \"something\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa9f7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115, 111, 109, 101, 116, 104, 105, 110, 103]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but in later future get to know that we can directly use the encoding done in the bytes like utf-8 or utf-16 \n",
    "#but here is the famous utf-8 enclding schema\n",
    "list(\"something\".encode(\"utf-8\"))\n",
    "# see its the same as above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ed570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But we can't just feed this into the attention trasformer casue the seq-len will be very long for short or some long data \n",
    "#and will not be as effective and the network wil become long for just small sets of data and it will not learn as \n",
    "# it will learn from a set of words insted of alphabets encoding \n",
    "# and thats where byte-pair encoding comes at \n",
    "# eg we have a string \"aabbccbbcccaa\" its vocab size is 3 cause there are only a,b,c and seq_len/context-len is 13 \n",
    "# we can make this thing/seq-len shorter by making the vocab size bigger here is an example\n",
    "# Z= aa,X=bb,Y=cc\n",
    "#now the string is = \"ZXYZYZ\" but the vocab size is a,b,c,X,Y,Z\n",
    "#we can make it even more shorter if we see some common pairs \n",
    "#O=YZ\n",
    "# \"ZXOO\" see the seq_len/context-len became shorter from 13 to 4 but the vocab size is a,b,c,X,Y,Z,O but this traid off is ok \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a3ffc913-a00c-4c99-ab2a-fe79741b8b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab of data : 46\n"
     ]
    }
   ],
   "source": [
    "#here's how to do it \n",
    "# first we take data from somewhere \n",
    "with open(\"input.txt\",\"r\",encoding=\"utf-8\")  as f:\n",
    "    texts = f.read()\n",
    "data = texts[:1000]\n",
    "print(f\"vocab of data : {len(set(data))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5171e83d-5bae-486a-9276-2ebcc04150ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 105, 114, 115, 116]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens= data.encode(\"utf-8\")\n",
    "tokens = list(map(int,tokens))\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "366b1f4a-6da6-45b0-aeaa-adda1400eb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now we'll have to make pair encoding of these binary utf-8 representation\n",
    "def get_stat(data):\n",
    "    output = {}\n",
    "    for pair in zip(data,data[1:]):\n",
    "        output[pair] = output.get(pair,0)+1\n",
    "    return output\n",
    "stats =get_stat(tokens)\n",
    "# sorted(((v,k) for k,v in stats.items()),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0fa32ad5-88bd-4e1b-8ccc-08ae213ad776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e ', (101, 32))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats,key=stats.get)\n",
    "(bytes(top_pair)).decode(\"utf-8\"),top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "692b7ed8-58fc-4995-9585-200ae5627bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge(data,pair,idx):\n",
    "    new_data = []\n",
    "    i = 0\n",
    "    while i<len(data):\n",
    "        if i< len(data)-1 and pair[0]==data[i] and pair[1]==data[i+1]:\n",
    "            new_data.append(idx)\n",
    "            i+=2\n",
    "        else:\n",
    "            new_data.append(data[i])\n",
    "            i+=1\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5bb900c2-6d09-442c-a860-f5e5f4306904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "11af24b4-f185-44b8-a692-cdb62f0d17b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size =  200\n",
    "num_merge = vocab_size-max(tokens)\n",
    "ids = list(tokens)\n",
    "merges = {}\n",
    "for i in range(num_merge):\n",
    "    stats = get_stat(ids)\n",
    "    pair = max(stats,key=stats.get)\n",
    "    idx = 123+i\n",
    "    ids = merge(ids,pair,idx)\n",
    "    merges[pair]=idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "66b2c896-c6ec-4585-92f3-9d2105089055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding it \n",
    "vocab= {idx: bytes([idx]) for idx in range(200)}\n",
    "for (p0,p1),idx in merges.items():\n",
    "    vocab[idx] =vocab[p0]+vocab[p1]\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[ix] for ix in ids)\n",
    "    text = tokens.decode(\"utf-8\")\n",
    "    return text\n",
    "decode([119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "38b19bf6-b84e-458b-9294-0a6bc1703e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 109, 108, 111]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding\n",
    "def encoding(ids):\n",
    "    tokens = list(ids.encode(\"utf-8\"))\n",
    "    while len(tokens) >=2:\n",
    "        stats = get_stat(tokens)\n",
    "        pair = min(stats,key=lambda p: merges.get(p,float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens,pair,idx)\n",
    "    return tokens\n",
    "encoding(\"hemlo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0f777386-6e4d-4447-b349-fa7734ad8fc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e ',\n",
       " 'th',\n",
       " 't ',\n",
       " 's ',\n",
       " ', ',\n",
       " 'en',\n",
       " 'it',\n",
       " 'r ',\n",
       " ':\\n',\n",
       " 'd ',\n",
       " '\\n\\n',\n",
       " 'an',\n",
       " 'ou',\n",
       " 'iti',\n",
       " 'itiz',\n",
       " 'itizen',\n",
       " 're',\n",
       " 'ir',\n",
       " 'irs',\n",
       " 'the',\n",
       " '.\\n\\n',\n",
       " 'll',\n",
       " 'ar',\n",
       " 'to',\n",
       " 'Firs',\n",
       " 'Citizen',\n",
       " 'Citizen:\\n',\n",
       " 'y ',\n",
       " 'pe',\n",
       " 'es',\n",
       " 'us ',\n",
       " 'the ',\n",
       " 'n ',\n",
       " 'First ',\n",
       " 'First Citizen:\\n',\n",
       " 'or',\n",
       " 'we ',\n",
       " 'ak',\n",
       " 've',\n",
       " 'to ',\n",
       " 'no',\n",
       " 'is ',\n",
       " 'ic',\n",
       " 'peak',\n",
       " 'All',\n",
       " 'All:\\n',\n",
       " '.\\n\\nFirst Citizen:\\n',\n",
       " 'ol',\n",
       " 'kno',\n",
       " 'know',\n",
       " 'ie',\n",
       " ': ',\n",
       " 'or ',\n",
       " 'speak',\n",
       " 'are ',\n",
       " 'll ',\n",
       " 'olve',\n",
       " 'mi',\n",
       " 'f ',\n",
       " 'le',\n",
       " \"'t\",\n",
       " 'our ',\n",
       " 'in',\n",
       " 'on',\n",
       " 'y, ',\n",
       " 'ec',\n",
       " 'go',\n",
       " 'su',\n",
       " 'ld ',\n",
       " 'bu',\n",
       " 'er',\n",
       " 'ven',\n",
       " 'in ',\n",
       " 'for ',\n",
       " 'ore ',\n",
       " 'pr',\n",
       " 'ce',\n",
       " 'ed ']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[decode([p0,p1]) for (p0,p1),v in merges.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff470165-eb84-4fee-a672-9e3a5805c6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
