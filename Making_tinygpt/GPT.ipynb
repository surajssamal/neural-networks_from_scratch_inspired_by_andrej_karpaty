{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83144a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93088d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a428b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fdc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\",\"r\",encoding=\"utf-8\")  as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b064917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is the length of the no of texts :-  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"here is the length of the no of texts :- \",len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "942a8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(texts[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e64403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 65\n"
     ]
    }
   ],
   "source": [
    "#now making characters out of those list\n",
    "chars = sorted(list(set(texts)))\n",
    "vocab_size = len(chars)\n",
    "n_embed =32 # its for nn not for biagram so dont get confused \n",
    "print(\"\".join(chars),vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c67346",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_string = {s:i for s,i in enumerate(chars)}\n",
    "string_int = {i:s for s,i in enumerate(chars)}\n",
    "encoder = lambda x:[ string_int[s] for s in x]# encoder \n",
    "decoder = lambda x: \"\".join([int_string[s] for s in x]) # and decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3361ee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 63, 43, 1, 40, 57, 42, 49, 63, 1, 57, 53, 51, 43, 1, 58, 46, 47, 52, 45, 1, 57, 53, 51, 43, 58, 46, 47, 52, 45, 1, 57, 53, 51, 43, 58, 46, 47, 52, 45]\n",
      "\n",
      "\n",
      "[726, 68, 275, 21282, 2584, 617, 1517, 1223, 1223]\n"
     ]
    }
   ],
   "source": [
    "# so there is process of encoding and decoding the vocabs the current encode process we're using is the general 65 vocab length generic character in the dataset\n",
    "# but usually GPT2 use Byte pair encoding tokanisation which is bit has more vocab size like 50000 characters insted of 65 character \n",
    "#the traid off b/w both of them is that which the no of character in a sentence is large the 64 vocab size length of sequence is huge \n",
    "#but in byte paired endcoding tokeniser the same thing uses less #here is the example\n",
    "\n",
    "\n",
    "#with generic encoding decoding \n",
    "some_character= \"oye bsdky some thing something something\"\n",
    "print(encoder(some_character))\n",
    "# with byte paired endcoding tokeniser\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"\\n\")\n",
    "print(enc.encode(some_character))\n",
    "# see the sequence in tiktoken is not too long as the generic way of doing it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a938c725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]),\n",
       " torch.int64,\n",
       " tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "          1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "         57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "          6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "         58, 47, 64, 43, 52, 10,  0, 37, 53, 59]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =  torch.tensor(encoder(texts),dtype=torch.long)\n",
    "data.shape,data.dtype,data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a01100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now spliting the data\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "validation_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b8d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) : 47\n",
      "when input is tensor([18, 47]) : 56\n",
      "when input is tensor([18, 47, 56]) : 57\n",
      "when input is tensor([18, 47, 56, 57]) : 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) : 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) : 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) : 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) : 58\n"
     ]
    }
   ],
   "source": [
    "# now in this we'll use another approach just like the n-gras but till 8 or any number you wish \n",
    "#here is the example\n",
    "batch_size = 12\n",
    "block_size =8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"when input is {context} : {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79640440-07be-4853-81cb-2cbfb1d46eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else validation_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x = torch.stack([data[i:block_size+i] for i in ix])\n",
    "    y = torch.stack([data[i+1:block_size+i+1] for i in ix])\n",
    "    x,y =x.to(device),y.to(device)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537c73a5-87a7-49ac-b07c-afcd508782c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nShlnlIlIgrQgLjpHcJM$Xuo-rOIxfKqotiDvB;sHiGUp$ACGvXlJbMkJRL'tbziRqdv,Z vtL!moNxHggnt\\nm?s,Tq&,-QnBhhMA\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making Normal biagram\n",
    "class biagram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #making an embedding lookup table\n",
    "        self.token_embedding = nn.Embedding(vocab_size,vocab_size)\n",
    "    def forward(self,x,y=None):\n",
    "        logits = self.token_embedding(x)\n",
    "        if y is  None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            y= y.view(B*T)\n",
    "            #calculating loss\n",
    "            loss = F.cross_entropy(logits,y)\n",
    "        \n",
    "        return logits,loss \n",
    "    def generate(self,x,tokens):\n",
    "        for i in range(tokens):\n",
    "            logits,loss = self(x)            \n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=1)\n",
    "            next_x = torch.multinomial(probs,num_samples=1)\n",
    "            x = torch.cat((x,next_x),dim=1)\n",
    "            \n",
    "        return x\n",
    "xb,yb = get_batch(\"train\")\n",
    "xb,yb =xb.to(device),yb.to(device)\n",
    "b1 = biagram().to(device=device)\n",
    "logits,loss = b1(xb,yb)\n",
    "\n",
    "ix = torch.zeros((1,1),dtype=torch.long).to(device)\n",
    "prediction = decoder(b1.generate(ix,tokens=100)[0].tolist())\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1575622d-2f3a-4bd9-ba87-daf5fa755865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5641186237335205\n",
      "\n",
      "TENGLAnowomur amot herou omowect?\n",
      "ILow;\n",
      "KEGEOFon tw INSelck ple ld LI budoupat Win\n",
      "my Fot rt INT:--o\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(b1.parameters(),lr=1e-2)\n",
    "\n",
    "for i in range(4000):\n",
    " \n",
    "    xb,yb =get_batch(\"train\")\n",
    "    # evaluatiing the loss\n",
    "    optimizer.zero_grad() #turning the grads to zero \n",
    "    logits,loss = b1(xb,yb)\n",
    "    loss.backward()\n",
    "    optimizer.step() #updating the values \n",
    "print(loss.item())\n",
    "prediction = decoder(b1.generate(ix,tokens=100)[0].tolist())\n",
    "print(prediction )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a8d3f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NN_method(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the the logits from the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)# vocab_size.n_embed is global so thats where we are fetching it from\n",
    "        #we're also taking its possition embeding so herse is how to do it \n",
    "        self.position_embeding_table = nn.Embedding(block_size,n_embed)\n",
    "        #creating a nn \n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "\n",
    "    def forward(self,x,y=None):\n",
    "        B,T =x.shape\n",
    "        token_embedding = self.token_embedding_table(x)\n",
    "        #using the postion embedding \n",
    "        position_emb = self.position_embeding_table(torch.arange(T,device=device))\n",
    "        x = token_embedding+position_emb\n",
    "    \n",
    "        logits = self.lm_head(x)\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #scaling it into 2d from  3d\n",
    "            B,T,C =logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits,y)\n",
    "        return logits,loss\n",
    "    def generate(self,x,max_new_token):\n",
    "        # x is (B,T) array of indices  in the current context\n",
    "        for _ in range(max_new_token):\n",
    "            ix = x[:,-block_size:]\n",
    "            #get the prediction\n",
    "            logits,loss =self(ix)\n",
    "            #focus on the last time stamp\n",
    "            logits = logits[:,-1,:] #becomes(B,C)\n",
    "            #apply softmax to get probibilties\n",
    "            probs = F.softmax(logits,dim=1)\n",
    "            #sample from the distribution\n",
    "            x_next = torch.multinomial(probs,num_samples=1) #(B,1)\n",
    "            #append sample index to running sequence \n",
    "            x =torch.cat((x,x_next),dim=1) #(B,T+1)\n",
    "        return x\n",
    "model = NN_method().to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd0890e6-09a4-401d-b861-2f4b6b5bebac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$Gjrn\\nqU??'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = torch.zeros((1,1),dtype=torch.long).to(device)\n",
    "pred = model.generate(x=ix,max_new_token=10)\n",
    "prediction = decoder(pred[0].tolist())\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3af7207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LOSS evaluation class \n",
    "class Loss_evaluation:\n",
    "    def __init__(self,model,epocs:int):\n",
    "        self.epocs =epocs\n",
    "        self.eval_iter =300\n",
    "        self.model =model\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in [\"train\",\"val\"]:\n",
    "            losses = torch.zeros(self.eval_iter)\n",
    "            for k in range(self.eval_iter):\n",
    "                X,Y = get_batch(split)\n",
    "                logits,loss = self.model(X,Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split]= losses.mean()\n",
    "        self.model.train()\n",
    "        return out\n",
    "    def loss(self):\n",
    "        for i in range(self.epocs):\n",
    "            if i%self.eval_iter==0:\n",
    "                losses=self.estimate_loss()\n",
    "                print(f\"step {i} : train {losses['train']}, test : {losses['val']}\")\n",
    "                #sample a batch of data \n",
    "            xb,yb =get_batch(\"train\")\n",
    "    \n",
    "            # evaluatiing the loss\n",
    "            logits,loss = self.model(xb,yb)\n",
    "            optimizer.zero_grad(set_to_none=True) #turning the grads to zero \n",
    "            loss.backward()\n",
    "            optimizer.step() #updating the values \n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c8026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : train 4.415390491485596, test : 4.404302597045898\n",
      "step 300 : train 2.992036819458008, test : 2.993666410446167\n",
      "step 600 : train 2.754615545272827, test : 2.772991418838501\n",
      "step 900 : train 2.6750941276550293, test : 2.681922197341919\n",
      "2.717660665512085\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "Loss_evaluation(model,1000).loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "149f17f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X an\n",
      "B, ileryou.\n",
      "Sthalomitoutheedrilyof:\n",
      "Tke, he fr'ear per:\n",
      "Ss I japps!sh\n",
      "HjcBBowseru:\n",
      "Neo!ef, ce sy IOvve\n",
      "OO\n",
      "\n",
      "mtds imat, mye d, t  ghllyeth.\n",
      "Fe li,\n",
      "e:  chayoro bs ovucotenMty s'sin serTpfo.\n",
      "Py t, bo\n"
     ]
    }
   ],
   "source": [
    "# now generating the words\n",
    "\n",
    "ix = torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "print(decoder(model.generate(ix,max_new_token=200)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820235b2",
   "metadata": {},
   "source": [
    "## Mathmatical trick for self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285fc523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 5]),\n",
       " tensor([[ 1.0078,  1.5814,  1.4993,  0.8910, -1.0526],\n",
       "         [ 0.7135,  0.1649,  0.2068,  0.6118, -1.3212],\n",
       "         [-0.4626,  0.1551, -0.2281,  0.4731, -1.1227],\n",
       "         [-0.1096,  0.0230, -0.9178,  0.4050, -1.2176],\n",
       "         [-0.2847, -0.1277, -0.6658,  0.7190, -1.1999],\n",
       "         [ 0.0058,  0.1042, -0.8282,  0.9320, -0.8676],\n",
       "         [ 0.1251,  0.1211, -1.2125,  0.7020, -0.5038],\n",
       "         [ 0.2407, -0.0394, -1.1678,  0.5832, -0.5018]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we're gonna use self-attention \n",
    "# so how it works is this :- 1. we are totally dependent on the previous outcomes like at [BXTXC] where TXC is dependent on T-1XC OR can say T-2 OR T-0XC \n",
    "# and then we find average all previous words  of it and find the prediction\n",
    "\n",
    "# here's and axample how we do it \n",
    "\n",
    "B,T,C =4,8,5\n",
    "some_x =torch.randn((B,T,C))\n",
    "x_bag_of_words = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = some_x[b,:t+1]\n",
    "        x_bag_of_words[b,t] = torch.mean(x_prev,0)\n",
    "x_bag_of_words.shape,x_bag_of_words[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e4403",
   "metadata": {},
   "source": [
    "## anther way of doing it using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f7b51c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a   =\n",
      "tensor([[8., 2., 4., 2.],\n",
      "        [2., 5., 7., 6.],\n",
      "        [5., 2., 4., 5.]])\n",
      "b   =\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "dot product  =\n",
      "\n",
      "tensor([[ 8.,  2.,  4.,  2.],\n",
      "        [10.,  7., 11.,  8.],\n",
      "        [15.,  9., 15., 13.]])\n",
      "\n",
      "\n",
      "average  =\n",
      "tensor([[8.0000, 2.0000, 4.0000, 2.0000],\n",
      "        [5.0000, 3.5000, 5.5000, 4.0000],\n",
      "        [5.0000, 3.0000, 5.0000, 4.3333]])\n"
     ]
    }
   ],
   "source": [
    "# but there is another way of doing it using matrix multiplication with is very much efficient then doing the cell before trick \n",
    "#so here's an explanation for that :- \n",
    "a = torch.randint(1,9,(3,4)).float()\n",
    "b = torch.tril(torch.ones(3,3))\n",
    "print(f\"a   =\")\n",
    "print(a)\n",
    "print(f\"b   =\")\n",
    "print(b)\n",
    "print(f\"dot product  =\\n\")\n",
    "print(b@a)\n",
    "print(\"\\n\")\n",
    "# we did the sim part of the average by using the matrix multiplication but how to do the (sum_of_column)/no_of_column\n",
    "#lets do the no of columns part and there is a trick to do that here it is :-\n",
    "b =b/b.sum(1,keepdims=True)\n",
    "print(f\"average  =\")\n",
    "print(b@a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30a2455c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets check if it same as the matematical trick we did previously \n",
    "wei = torch.tril(torch.ones(8,8))\n",
    "wei = wei/wei.sum(1,keepdim=True)\n",
    "x_bag_of_words2 = wei@some_x\n",
    "torch.allclose(x_bag_of_words2,x_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d0e44",
   "metadata": {},
   "source": [
    "## there is another way using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b860a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so what we do is use softmax to find the trig veritcal partition of ones here's what i mean\n",
    "tril = torch.tril(torch.ones(8,8))\n",
    "wei = torch.zeros((8,8))\n",
    "wei = wei.masked_fill(tril==0,float(\"-inf\"))\n",
    "# now making the distribution of 1 in this\n",
    "wei = F.softmax(wei,dim=1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34b38ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can do the obvious what we did in matrix multiplication\n",
    "x_bag_of_words3 = wei@some_x\n",
    "torch.allclose(x_bag_of_words,x_bag_of_words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9244ce-fa43-4a8d-938f-3d039eeb45bf",
   "metadata": {},
   "source": [
    "##        ----------------------------------- Assembeling every thing ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b32c288c-eb08-46c6-ac16-80fdfc50320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the variables\n",
    "batch_size = 14\n",
    "block_size =100\n",
    "n_embed = 64\n",
    "num_head=4\n",
    "n_layer = 5\n",
    "dropout_percentage =0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2b9bacd-c1fb-4e05-842b-5d80c2ba2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else validation_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x = torch.stack([data[i:block_size+i] for i in ix])\n",
    "    y = torch.stack([data[i+1:block_size+i+1] for i in ix])\n",
    "    x,y =x.to(device),y.to(device)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d159e482-db3d-48ab-9ec5-8fb399268c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO lets make the attention block\n",
    "class Head(nn.Module):\n",
    "    #making single head of self attention \n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query= nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value  = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout_percentage)\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = k @ q.transpose(-2,-1) * C**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0,float(\"-inf\"))\n",
    "        wei =  F.softmax(wei,dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out =  wei @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4609cefc-6eac-421b-83c0-aef4d649b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Multihead attention\n",
    "class Multihead(nn.Module):\n",
    "    def __init__(self,No_of_Heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads =nn.ModuleList([Head(head_size) for _ in range(No_of_Heads)])\n",
    "        self.projection = nn.Linear(n_embed,n_embed)\n",
    "        self.dropout = nn.Dropout(dropout_percentage)\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([i(x) for i in self.heads],dim=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bd5fc45-2825-4693-a9f7-2a250d400b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a feed forward layer\n",
    "class Fedd_forward(nn.Module):\n",
    "    def __init__(self,dimension):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dimension,4*dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*dimension,dimension),\n",
    "            nn.Dropout(dropout_percentage),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9029f92-aac3-48a0-800e-7e9d2ae34667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a block of all those attention and feed forward layer \n",
    "class Block(nn.Module):\n",
    "    def __init__(self,num_head,n_embed):\n",
    "        super().__init__()\n",
    "        self.sa_head = Multihead(No_of_Heads=num_head,head_size=n_embed//num_head)\n",
    "        self.ffn = Fedd_forward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=x+ self.sa_head(self.ln1(x)) #We did the addition cause of the resedual connection\n",
    "        x =x+ self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b25d7f5-a9c8-4dee-bffa-9d69fa2c299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_method(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the the logits from the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)# vocab_size.n_embed is global so thats where we are fetching it from\n",
    "        #we're also taking its possition embeding so herse is how to do it \n",
    "        self.position_embeding_table = nn.Embedding(block_size,n_embed)\n",
    "        #creating a block of attention and FFN\n",
    "        self.block = nn.Sequential(*[Block(num_head,n_embed) for _ in range(n_layer)])\n",
    "        #layer norm\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        #creating a nn \n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "\n",
    "    def forward(self,x,y=None):\n",
    "        B,T =x.shape\n",
    "        token_embedding = self.token_embedding_table(x)\n",
    "        #using the postion embedding \n",
    "        position_emb = self.position_embeding_table(torch.arange(T,device=device))\n",
    "        x = token_embedding+position_emb\n",
    "        x= self.block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if y==None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #scaling it into 2d from  3d\n",
    "            B,T,C =logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits,y)\n",
    "        return logits,loss\n",
    "    def generate(self,x,max_new_token):\n",
    "        # x is (B,T) array of indices  in the current context\n",
    "        for _ in range(max_new_token):\n",
    "            ix = x[:,-block_size:]\n",
    "            #get the prediction\n",
    "            logits,loss =self(ix)\n",
    "            #focus on the last time stamp\n",
    "            logits = logits[:,-1,:] #becomes(B,C)\n",
    "            #apply softmax to get probibilties\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            #sample from the distribution\n",
    "            x_next = torch.multinomial(probs,num_samples=1) #(B,1)\n",
    "            #append sample index to running sequence\n",
    "            x =torch.cat((x,x_next),dim=1) #(B,T+1)\n",
    "\n",
    "        return x        \n",
    "        \n",
    "model1 = NN_method().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2b8f577-7f82-4943-b0e8-f4065cbaea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : train 4.319794654846191, test : 4.320187568664551\n",
      "step 300 : train 2.402920722961426, test : 2.4247000217437744\n",
      "step 600 : train 2.1075551509857178, test : 2.1550838947296143\n",
      "step 900 : train 1.9683036804199219, test : 2.056759834289551\n",
      "2.0003509521484375\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model1.parameters(),lr=1e-2)\n",
    "Loss_evaluation(epocs=1000,model=model1).loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a3d2e39-c1da-4f09-93a8-43e8392d32c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 263,873 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model1.parameters())\n",
    "print(f\"The model has {total_params:,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c06cfba1-b207-4a9b-9d81-b5f029339c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Noth faucte there,\n",
      "A dat for grhave dave enould freispen fontir he mangs man athow to krow,\n",
      "Vore in morre hen rand fite ev hafk,\n",
      "When mich chich hatt rase conyour.\n",
      "A wordestress compects, to more\n",
      "The come fith stis bent mauthece withen the thees wit!\n",
      "\n",
      "Sightn.\n",
      "\n",
      "MALIAwIUSCUS:\n",
      "Ans it and Stry, I bemm, feak, my Jus they geat gize care.\n",
      "\n",
      "PANGEL:\n",
      "A'n atitt of bot tue kin shelorcer, sinnot Crueemselys; I arthy canlody sonour'stres.\n",
      "Ay meln's tor slard? My of I\n",
      "Your thend or ithall s\n",
      "Luid Wascomicein; d\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch. long, device=device)\n",
    "print (decoder(model1.generate (context, max_new_token=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d700ea4-48d8-484d-8bfb-c43ce8df9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c51ac3e-e88d-4b59-a89e-2589c2273074",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1, 'model1_full.pth') #Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14defeaa-6a6e-4c29-9c48-547259cb5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15600/2226836182.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1new = torch.load('model1_full.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN_method(\n",
       "  (token_embedding_table): Embedding(65, 64)\n",
       "  (position_embeding_table): Embedding(100, 64)\n",
       "  (block): Sequential(\n",
       "    (0): Block(\n",
       "      (sa_head): Multihead(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): Fedd_forward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa_head): Multihead(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): Fedd_forward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa_head): Multihead(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): Fedd_forward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa_head): Multihead(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): Fedd_forward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa_head): Multihead(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): Fedd_forward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=64, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1new = torch.load('model1_full.pth')\n",
    "model1new.to(device)\n",
    "model1new.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f1ef7ee-6430-41a2-bf53-4d38718bacb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAOUCKICAVENCUSCARIIUS:\n",
      "O uSTIUS:\n",
      "LO, are sor,\n",
      "I'll bed hie leell hou; thoulds and turmffen,\n",
      "Not ie thish eet me riepnattrome\n",
      "He not a to Pated ther she morloo's pad.\n",
      "stacclrlibast, elf tib, matterew Jy chall a procome wast!\n",
      "\n",
      "Nakentent srer tand sow oughs mond so:\n",
      "I out oure heefar and forcannouldelmse Cor'd sicincily\n",
      "I word, not vong terelay your was be, A't\n",
      "brurk of all shim hiage he mell.\n",
      "Fortt a thavle he sriself Ere dount ands.\n",
      "Clist your\n",
      "That it you a hine nots a That fortiory,\n",
      "My thould prot and sit gre therake are,\n",
      "The idmencime that thy 's if reave your muct crendy the\n",
      "ch bant rut is arthougist chatud ther had sicraved thy\n",
      "of an it provems, a gatung of to thater\n",
      "To dombse asfy, shle anisg the brappumbexulin-for'stornt outh of\n",
      "Te womair lik ingle, somby spiare a Warl themof or edore:\n",
      "A done mike do firath bar, unhe we to has.\n",
      "\n",
      "AUSWARICK:\n",
      "Shou past thee wom go lordo modell, therer my sponoust thus.\n",
      "Such hemes, then is. That is his fe of-thout n ontrup low thy;\n",
      "Ay move lorss ulvel sump the all all eathict he fuls youche'\n",
      "Thare ought the lin you parthe of bett mve\n",
      "Bot thernit fat ad worthe are. I than,\n",
      "O kear to hat then: sweell's henfriant yed to mayse;\n",
      "Ent, well in mose ill well shoribe us, withse tim mote I dat premply;\n",
      "Fe yeet thet bure, biting gwith weare bessin ancelf.\n",
      "\n",
      "MATENUS:\n",
      "Clon for Packelild st our uny Cor i's unt, timmoy not me ouse break?\n",
      "\n",
      "SILAUVIARD:\n",
      "If mul and up or my there rour ther.\n",
      "\n",
      "QUEELONIET:\n",
      "O, yese; your havolds antsee\n",
      "Ho your not bea-ith;\n",
      "Welf daict whath are norseion is And of sall\n",
      "int and felst\n",
      "Thavak we now be manvant a ee 's tees doprovessicl?\n",
      "\n",
      "MEOMOUSTESTER:\n",
      "A thany may horriousinw make in yough gurest\n",
      "Aut stre; in'd we but abluct laxture-folsce!\n",
      "\n",
      "MENCENTENTIUS:\n",
      "Moustled you thou swormmove on be ablo\n",
      "And I shat beek stitk a sen orfuice prom wardont fuld rarstesmerely mank he in sounmunced best ouce?\n",
      "\n",
      "LAMESTIO:\n",
      "I wirt sonar do cone diely the womess gonce fall, somme?\n",
      "And it Srint-noble queselpter sh obly the Now post,\n",
      "Ulinn liet you\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, block_size), dtype=torch. long, device=device)\n",
    "print (decoder(model1.generate (context, max_new_token=2000)[0].tolist()[block_size:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3b0b8-7fc1-4b9a-b76a-ce2095656221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c643ef-e7d0-4ace-877f-fc7c20b9c806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
